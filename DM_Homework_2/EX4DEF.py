# -*- coding: utf-8 -*-
"""EX4Def.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L1trKeK1ENo5ym9I30EcEYcQVFWLewJF
"""

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import lower, regexp_replace, lit
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType
from pyspark.ml.feature import Tokenizer, StopWordsRemover
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number
from pyspark.sql import Row
import hashlib
import time
import sys
import itertools

# Initialize Spark
sc = pyspark.SparkContext('local[*]')
spark = SparkSession.builder.appName("ReadTSV").getOrCreate()

# Define the schema for your TSV file
schema = StructType([
    StructField("Product Description", StringType(), True),
    StructField("Price", DoubleType(), True),
    StructField("Prime Status", StringType(), True),
    StructField("Product URL", StringType(), True),
    StructField("Stars", DoubleType(), True),
    StructField("Number of Reviews", DoubleType(), True),
    StructField("Real rating", DoubleType(), True)
])

path = 'amazon_productsClean.tsv'  # Update with your actual file path

try:
    # Read the TSV file
    file_df = spark.read.option("multiline", True).option("delimiter", "\t").csv(
        path,
        header=True,
        schema=schema
    ).na.drop(how='any')

except Exception as e:
    print(f"Error: {e}")
    print("Please type the correct path to the jobs.tsv file:")
    path = input()
    file_df = spark.read.option("multiline", True).option("delimiter", "\t").csv(
        path,
        header=True,
        schema=schema
    ).na.drop(how='any')

df_clean = file_df.select('Product Description', (lower(regexp_replace('Product Description', "[^a-zA-Z\\s]", "")).alias('cleaned_text')))

# Tokenization
tokenizer = Tokenizer(inputCol='cleaned_text', outputCol='tokenized')
df_tokenized = tokenizer.transform(df_clean).select('Product Description', 'tokenized')

# Remove stop words
remover = StopWordsRemover(inputCol='tokenized', outputCol='no_stopwords')
df_no_stopwords = remover.transform(df_tokenized).select('Product Description', 'no_stopwords')

# Show the result without stemmed words
df_no_stopwords.show(truncate=False)

# Add docId (=row number)
window_spec = Window().orderBy(lit('A'))
df_with_doc_id = df_no_stopwords.withColumn("docId", row_number().over(window_spec)).select("Product Description", "no_stopwords", "docId")

data_rdd = df_with_doc_id.rdd.map(lambda row: (row["Product Description"], row["docId"]))

def create_shingles(description, k=10):
    return [description[i:i+k] for i in range(len(description)-k+1)]

# Apply the shingling function to create shingles RDD
shingles_rdd = data_rdd.map(lambda row: (row[1], create_shingles(row[0])))

def hash_family(i):
    result_size = 8
    # how many bytes we want back
    max_len = 20
    # how long can our i be (in decimal)
    salt = str(i).zfill(max_len)[-max_len:]

    def hash_member(x):
        sequence = x + salt
        return hashlib.sha1(sequence.encode("utf-8")).digest()[-result_size:]

    return hash_member

num_hash_functions = 100
hash_fn = hash_family(num_hash_functions)
hashed_shingles_rdd = shingles_rdd.map(lambda row: (row[0], [hash_fn(row[1][i]) for i in range(len(row[1]))]))

shingles_columns = ["docId", "hashed_shingles"]
df_shingles = hashed_shingles_rdd.map(lambda row: Row(docId=row[0], hashed_shingles=row[1])).toDF(shingles_columns)

# Print the schema and show the DataFrame
df_shingles.printSchema()
df_shingles.show(15, truncate=False)

start = time.time()
def min_hash(shingles, i):
    hash_fn = hash_family(i)
    min_signature = float('inf')  # big number
    for s in shingles:
        # some casts are needed
        hash_signature = int.from_bytes(hash_fn(str(s)), sys.byteorder)
        if hash_signature < min_signature:
            min_signature = hash_signature
    return min_signature
k=100
# Apply minHash function to create signatures RDD
signatures_rdd = hashed_shingles_rdd.map(lambda row: (row[0], [min_hash(row[1], i) for i in range(k)]))

# Take the first record
signatures_list = signatures_rdd.take(1)
end = time.time()

signatures_list = signatures_rdd.take(1)

# Display the result
for record in signatures_list:
    print(record)

print("Time spent for MinHashing: {}".format(end-start))

b=20
r=5
start = time.time()
band_lsh_rdd = signatures_rdd.map(lambda row: (row[0], [[row[1][i] for i in range(j*r, j*r+r)] for j in range(b)]))

hashed_bands_rdd = band_lsh_rdd.flatMap(lambda row: [(min_hash(tuple(row[1][j]), j), row[0]) for j in range(b)])

buckets_rdd = hashed_bands_rdd.map(lambda bucket: (bucket[0], [bucket[1]])).reduceByKey(lambda doc1, doc2: doc1 + doc2).filter(lambda collision: len(collision[1]) > 1)

def get_pairs(collisions_list):
    pair_list = []
    for pair in itertools.combinations(collisions_list, 2):
        pair_list.append(pair)
    return tuple(pair_list)

# Get pairs of (possible) near-duplicates
duplicates_rdd = buckets_rdd.flatMap(lambda pairs: get_pairs(pairs[1])).distinct()
end = time.time()
# Take the first 10 records
duplicates_list = duplicates_rdd.take(10)

# Display the result
for record in duplicates_list:
    print(record)
    product_description_with_doc1 = data_rdd.filter(lambda row: row[1] == record[0]).collect()
    product_description_with_doc2 = data_rdd.filter(lambda row: row[1] == record[1]).collect()
    print(product_description_with_doc1)
    print(product_description_with_doc2)

print("Time spent for LSH: {}".format(end-start))

joined_rdd = duplicates_rdd.join(data_rdd)

# Extract the product descriptions
descriptions_rdd = joined_rdd.map(lambda x: (x[1][0], x[1][1]))

# Collect and display the result
descriptions_list = descriptions_rdd.collect()

for record in descriptions_list:
    print(record)

# Take the first 10 records
number_of_duplicates = duplicates_rdd.count()
print("Number of duplicates:", number_of_duplicates)

hashed_shingles_rdd = df_shingles.rdd.map(lambda row: (row["docId"], row["hashed_shingles"])).map(lambda b: (b[0], [bytes(i) for i in b[1]]))

start = time.time()
cartesian_product_rdd = hashed_shingles_rdd.cartesian(hashed_shingles_rdd).filter(lambda tup: tup[0][0] != tup[1][0])

jaccard_rdd = cartesian_product_rdd.map(lambda x: (x[0][0], x[1][0], float(len(set(x[0][1]).intersection(x[1][1]))) / float(len(set(x[0][1]).union(x[1][1])))))

similar_pairs_rdd = jaccard_rdd.filter(lambda p: p[2] > 0.8).map(lambda t: (t[0], t[1])).map(lambda s: tuple(sorted(s))).distinct()
end = time.time()
print("Time spent for brute force comparisons: {}".format(end-start))
similar_pairs_list = similar_pairs_rdd.take(10)
for record in similar_pairs_list:
    print(record)
    product_description_with_doc1 = data_rdd.filter(lambda row: row[1] == record[0]).collect()
    product_description_with_doc2 = data_rdd.filter(lambda row: row[1] == record[1]).collect()
    print(product_description_with_doc1)
    print(product_description_with_doc2)

dim_of_jaccard = similar_pairs_rdd.count()

print("Number of distinct similar pairs:", dim_of_jaccard)
intersection_rdd = similar_pairs_rdd.intersection(duplicates_rdd).distinct()

# Take the first 10 records
intersection_list = intersection_rdd.take(10)

# Display the result
for record in intersection_list:
    print(record)
    dim_of_intersection = intersection_rdd.count()
print("Number of distinct values in the intersection:", dim_of_intersection)

number_of_documents = df_clean.count()
print("Number of documents:", number_of_documents)

false_negative = dim_of_jaccard - dim_of_intersection
true_positive = dim_of_jaccard
false_positive = number_of_duplicates - dim_of_intersection
true_negative = number_of_documents * (number_of_documents - 1) / 2

# Calculate false positive rate and false negative rate
false_positive_rate = false_positive / (false_positive + true_negative)
false_negative_rate = false_negative / (false_negative + true_positive)

# Print the results
print("False positive rate: {}\nFalse negative rate: {}".format(false_positive_rate, false_negative_rate))
